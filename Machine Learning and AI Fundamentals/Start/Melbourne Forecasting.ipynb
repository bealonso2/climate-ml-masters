{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melbourne Temperature Forecasting\n",
    "\n",
    "Welcome to another deep learning project. In this project, we will be forecasting the temperature in Melbourne using the Long Short-Term Memory (LSTM) model. The dataset contains the temperature data from 1981 to 1990. The dataset contains the following columns:\n",
    "\n",
    "1. Date: The date of the observation\n",
    "2. Temp: The temperature in Melbourne\n",
    "\n",
    "The dataset is available at the following link: \n",
    "https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\n",
    "\n",
    "or on the Kaggle website:\n",
    "https://www.kaggle.com/paulbrabban/daily-minimum-temperatures-in-melbourne\n",
    "\n",
    "The required libraries will be listed in the next cell for you to install if you don't have them already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q pandas matplotlib numpy scikit-learn torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the first project in which we are going to be exposed to temporal (time series) data. Recurrent Neural Networks (RNN) are the most popular models for time series forecasting. The Long Short-Term Memory (LSTM) model is a type of RNN that is capable of learning long-term dependencies. But more on the anatomy of the LSTM model later.\n",
    "\n",
    "For now, we need to load the data, preprocess it, and visualize the data to understand the patterns.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "In the next cell:\n",
    "- Load the dataset into a pandas dataframe\n",
    "    - Set the 'Date' column as the index\n",
    "    - Parse the dates, meaning convert the 'Date' column to datetime format\n",
    "- Plot the data to visualize the temperature patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset by creating a DataFrame and parsing the dates\n",
    "# Set the Date column as the index\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\"\n",
    "df = \n",
    "\n",
    "# Plot the data\n",
    "df.plot(figsize=(10, 5))\n",
    "plt.title(\"Daily Minimum Temperatures in Melbourne\")\n",
    "plt.ylabel(\"Temperature (Celsius)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if we can train a model to learn the cyclic patterns in the temperature data and forecast the daily minimum temperature in Melbourne.\n",
    "\n",
    "If you take a look at the data which you've just plotted, you'll notice that the temperature ranges from 0 to 26 degrees. However, it is good practice to normalize the data before feeding it to the model. This is because the LSTM model is sensitive to the scale of the input data just like most deep learning models.\n",
    "\n",
    "We will use Scikit-Learn's MinMaxScaler to scale the data. The MinMaxScaler scales the data to a fixed range of 0 to 1 by default, but we will scale it to a range of -1 to 1. This is because the LSTM model uses the hyperbolic tangent activation function, which outputs values in the range of -1 to 1.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "We need to scale the data and split it into training and testing sets. LSTM models are fed sequences of data, so we need to create sequences of data to feed to the model. We will use the `TemperatureDataset` class to create PyTorch datasets and dataloaders for the training and testing sets.\n",
    "\n",
    "A sequence of data, in this case, will be a sequence of 30 days of temperature data. The model will be trained to forecast the temperature on the 31st day given the previous 30 days of temperature data.\n",
    "\n",
    "We need to implement a custom dataset to make it easier to create sequences of data. When an item is requested from `TemperatureDataset`, it should return a sequence of 30 days (X) of temperature data and the temperature on the 31st day (y).\n",
    "\n",
    "In the next cell:\n",
    "- Scale the data using the MinMaxScaler\n",
    "    - Scale the data to a range of -1 to 1\n",
    "- Add the scaled data to a new column in the dataframe\n",
    "- Split the data into training and testing sets\n",
    "    - Use the first 80% of the data for training and the remaining 20% for testing\n",
    "- Use the provided `TemperatureDataset` class to create PyTorch datasets and dataloaders for the training and testing sets\n",
    "    - Set the batch size\n",
    "    - Shuffle the training dataset sequences (not the testing dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Preprocess the data by normalizing the temperature values using MinMaxScaler\n",
    "# which scales the values between -1 and 1\n",
    "scaler = \n",
    "\n",
    "# Add a new column to the DataFrame with the normalized values\n",
    "df[\"Temp Transformed\"] = \n",
    "\n",
    "# Convert the dataframe to a numpy array\n",
    "all_data = df[\"Temp Transformed\"].values.astype(float)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "train_size = int(len(all_data) * 0.8)\n",
    "train_data = \n",
    "test_data = \n",
    "\n",
    "\n",
    "# Define a PyTorch Dataset class for sequence generation\n",
    "class TemperatureDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        # Initialize the dataset with data and sequence length\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset, which is the total number of data points\n",
    "        # minus the sequence length to ensure we have enough data for the last sequence\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get a sequence of data points starting from the given index\n",
    "        x = self.data[index : index + self.seq_length]\n",
    "\n",
    "        # Get the target value which is the next data point after the sequence\n",
    "        y = self.data[index + self.seq_length]\n",
    "\n",
    "        # Convert the sequence and target to PyTorch tensors with float32 data type\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(\n",
    "            y, dtype=torch.float32\n",
    "        )\n",
    "\n",
    "\n",
    "# Set sequence length and batch size\n",
    "seq_length = 30\n",
    "batch_size = 32\n",
    "\n",
    "# Create training and testing datasets using the TemperatureDataset class\n",
    "train_dataset =\n",
    "test_dataset =\n",
    "\n",
    "# Create DataLoaders for training and testing datasets\n",
    "# Shuffle the training dataset and keep the testing dataset in order\n",
    "# Use the batch size defined above\n",
    "train_loader =\n",
    "test_loader ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "We've use the batch size parameter a couple of times now. The batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters in the training process. A small batch size can lead to a noisy gradient update, while a large batch size can lead to a slow convergence. The batch size is a trade-off between the accuracy of the model and the speed of training.\n",
    "\n",
    "## The LSTM Model\n",
    "\n",
    "Now we need to define the LSTM model. The LSTM model is a type of RNN that is capable of learning long-term dependencies. The LSTM model has three gates: the input gate, the forget gate, and the output gate. The input gate controls the amount of new information to be stored in the cell state. The forget gate controls the amount of information to be discarded from the cell state. The output gate controls the amount of information to be output from the cell state.\n",
    "\n",
    "To implement our LSTM model, we will use the `nn.LSTM` module provided by PyTorch to learn the temperature patterns in the data and a fully connected layer to make the temperature forecast (linear regression).\n",
    "\n",
    "In the forward pass, the LSTM model will take the sequence of 30 days of temperature data as input and produce output features at each time step. We will only use the output features at the last time step to make the temperature forecast with the fully connected layer.\n",
    "\n",
    "We will not use the hidden state or cell state of the LSTM model in this project because we are only interested in forecasting the final value of the sequence.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "In the next cell:\n",
    "\n",
    "- Define the LSTM model\n",
    "    - Use the `nn.LSTM` module to define the LSTM layer\n",
    "        - Use the `batch_first=True` parameter to set the input tensor shape as (batch_size, seq_len, input_size)\n",
    "    - Use the `nn.Linear` module to define the fully connected layer for the temperature forecast\n",
    "    - Define the forward pass of the model\n",
    "        - Use the LSTM layer to learn the temperature patterns in the data\n",
    "        - Use the fully connected layer to make the temperature forecast with the output features at the last time step\n",
    "- Use the Mean Squared Error (MSE) loss function to calculate the loss\n",
    "- Use the Adam optimizer to update the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Set device to GPU if available (this will speed up the training)\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define an LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=50, output_size=1):\n",
    "        # Initialize the superclass\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Save the hidden layer size\n",
    "        self.hidden_layer_size = \n",
    "\n",
    "        # Define the LSTM layer with input size, hidden layer size, and batch_first=True\n",
    "        self.lstm = \n",
    "\n",
    "        # Define the output layer\n",
    "        self.linear = \n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        # LSTM expects input of shape (batch_size, seq_length, num_features)\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "\n",
    "        # Take the output from the last timestep\n",
    "        predictions = \n",
    "\n",
    "        # Return the output predictions\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialize the model and send it to the device\n",
    "model = \n",
    "\n",
    "# Initialize the mean squared error loss function\n",
    "loss_function = \n",
    "\n",
    "# Use Adam optimizer with learning rate of 0.001 and weight decay of 1e-5\n",
    "optimizer = \n",
    "\n",
    "# Gradient clipping to stabilize training\n",
    "clip_value = 1.0\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0  # Initialize the total loss for the epoch\n",
    "    for seq, labels in train_loader:\n",
    "        # Move data to the correct device\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "\n",
    "        # Reshape input to (batch_size, seq_length, input_size)\n",
    "        seq = seq.view(-1, seq_length, 1)\n",
    "        labels = labels.view(-1, 1)  # Reshape labels to match prediction output size\n",
    "\n",
    "        # Zero the gradients\n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = \n",
    "\n",
    "        # Compute the loss\n",
    "        loss = \n",
    "\n",
    "        # Backward pass\n",
    "        \n",
    "\n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        # Update the model parameters by stepping the optimizer\n",
    "        \n",
    "\n",
    "        # Add the batch loss to the epoch loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss}\")\n",
    "\n",
    "# Testing loop\n",
    "model.eval()  # Set model to evaluation mode\n",
    "test_losses = []\n",
    "with torch.no_grad():\n",
    "    for seq, labels in test_loader:\n",
    "        # Move data to the correct device\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "\n",
    "        # Reshape input to (batch_size, seq_length, input_size)\n",
    "        seq = seq.view(-1, seq_length, 1)\n",
    "        labels = labels.view(-1, 1)  # Reshape labels to match prediction output size\n",
    "\n",
    "        y_pred = model(seq)\n",
    "        loss = loss_function(y_pred, labels)\n",
    "        test_losses.append(loss.item())\n",
    "\n",
    "avg_test_loss = np.mean(test_losses)\n",
    "print(f\"Test loss: {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "You may notice some comments and pieces of code related to exploding gradients. The exploding gradient problem occurs when the gradients grow exponentially as they are backpropagated through the network. This can cause the model to fail to learn the patterns in the data.\n",
    "\n",
    "We fought this using gradient clipping. Gradient clipping is a technique to prevent the gradients from growing too large during training. We can clip the gradients to a maximum value (like 1.0) to prevent them from growing too large.\n",
    "\n",
    "We also used the weight decay parameter in the optimizer to prevent the model from learning very large weights. The weight decay parameter is a regularization term that penalizes large weights in the model which can lead to overfitting.\n",
    "\n",
    "Average loss (as opposed to total loss) is a more useful metric for comparing models. The average loss is the total loss divided by the number of batches. This gives us the average loss per batch which is more interpretable than the total loss.\n",
    "\n",
    "## Generating Predictions and Evaluating the Model\n",
    "\n",
    "We've already used our test data to evaluate the model during training and compute an average test loss. Now let's generate predictions on our entire dataset.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "In the next cell:\n",
    "- Create a tensor from the entire dataset\n",
    "- In the training loop:\n",
    "    - Extract the sequence of 30 days of temperature data from the entire dataset\n",
    "    - Make a temperature forecast with the LSTM model\n",
    "    - Append the forecast to the predictions list\n",
    "- Clip the actual data by the sequence length so it is shifted to match the forecasted data\n",
    "- Invert the scaling of the forecasted and actual data using the MinMaxScaler we created earlier and its `inverse_transform` method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set model to evaluation mode\n",
    "\n",
    "\n",
    "# Prepare data for prediction by converting to PyTorch tensor and moving to device\n",
    "all_data_tensor = \n",
    "predictions = []\n",
    "\n",
    "# Use sliding window to predict temperatures across the whole dataset\n",
    "with torch.no_grad():\n",
    "    for i in range(len(all_data) - seq_length):\n",
    "        # Extract a sequence from the data\n",
    "        seq = \n",
    "\n",
    "        # Make prediction\n",
    "        y_pred = \n",
    "\n",
    "        # Save the prediction\n",
    "        predictions.append(y_pred.item())\n",
    "\n",
    "# Prepare the actual data (shifted to match the predictions length)\n",
    "actual_data = \n",
    "\n",
    "# Inverse scaling of the data\n",
    "predictions_inv = \n",
    "actual_data_inv = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Predictions\n",
    "\n",
    "Now all that is left to do is to visualize the predictions of the LSTM model! If you want, you can also calculate the Mean Squared Error (MSE) of the predictions to display in the title of the plot.\n",
    "\n",
    "In the next cell:\n",
    "- (Optional) Calculate the Mean Squared Error (MSE) of the predictions\n",
    "- Get the dates starting at the sequence length (for your x-axis)\n",
    "- Plot the actual and forecasted temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean squared error\n",
    "mse = \n",
    "\n",
    "# Convert x axis back to original dates by extracting dates starting from seq_length\n",
    "# from the original dataframe index\n",
    "dates = \n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(dates, actual_data_inv, label=\"Actual Temperature\")\n",
    "plt.plot(dates, predictions_inv.flatten(), label=\"Predicted Temperature\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Temperature (Celsius)\")\n",
    "plt.title(f\"Actual vs Predicted Daily Temperatures\\nMean Squared Error: {mse:.4f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "We can see that the LSTM model has learned the cyclic patterns in the temperature data and made accurate temperature forecasts. The LSTM model is a powerful model for time series forecasting because it can learn long-term dependencies in the data. The LSTM model is widely used in various applications such as speech recognition, language modeling, and machine translation.\n",
    "\n",
    "Notice we did not overfit the model—the forecasted data closely follows the actual data but does not predict outliers.\n",
    "\n",
    "### Recap\n",
    "\n",
    "In this project, we learned how to:\n",
    "\n",
    "- Load and preprocess time series data\n",
    "- Create sequences of data for the LSTM model\n",
    "- Define and train an LSTM model\n",
    "- Generate predictions and evaluate the model\n",
    "- Visualize the predictions and denormalize the data\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "If you are interested in learning more about time series forecasting, try to play around with the hyperparameters of the LSTM model. When you're done, continue on to the next project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
